{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation & Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "First, we need to load TensorFlow and setup the basic parts of the graph - inputs (a_0, y), and states (w_1, b_1, w_2, b_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "a_0 = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "middle = 30\n",
    "w_1 = tf.Variable(tf.truncated_normal([784, middle]))\n",
    "b_1 = tf.Variable(tf.truncated_normal([1, middle]))\n",
    "w_2 = tf.Variable(tf.truncated_normal([middle, 10]))\n",
    "b_2 = tf.Variable(tf.truncated_normal([1, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sigmoid function, although provided by TensorFlow’s extensive function library, is brought here as reference:\n",
    "\n",
    "$$\n",
    "\\sigma \\left ( x \\right )= \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return tf.div(tf.constant(1.0),\n",
    "                  tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "Provided that the input image is given by the $a_0$ matrix, calculating forward propagation for multiple images at a time can be done with simple matrix multiplication, defined as such:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& z_1 = a_0 \\cdot w_1 + b_1 \\\\\n",
    "& a_1 = \\sigma(z_1) \\\\\n",
    "& z_2 = a_1 \\cdot w_2 + b_2 \\\\\n",
    "& a_2 = \\sigma(z_2) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Given a tensor of multiple images, this can done in TensorFlow for all them at the same time (thanks to ‘broadcasting’), so the above gets a one-to-one transltion in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_1 = tf.add(tf.matmul(a_0, w_1), b_1)\n",
    "a_1 = sigma(z_1)\n",
    "z_2 = tf.add(tf.matmul(a_1, w_2), b_2)\n",
    "a_2 = sigma(z_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference\n",
    "The input provides $y$ as the test for the accuracy of the network’s output, so we compute the following vector:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\nabla a = a_2 - y \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff = tf.subtract(a_2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sigmoid prime function\n",
    "Here’s the derivate of the sigmoid function from above, which will be needed during the backward propagation:\n",
    "\n",
    "$$ \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmaprime(x):\n",
    "    return tf.multiply(sigma(x), tf.subtract(tf.constant(1.0), sigma(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "The most complicated part is the backward propagation. First, we need to compute the deltas of the weights and biases and then we descibe the algorithm in a functional, stateless way.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\nabla z_2 = \\nabla a \\cdot \\sigma'(z_2) \\\\\n",
    "& \\nabla b_2 = \\nabla z_2 \\\\\n",
    "& \\nabla w_2 = a_1^T \\cdot \\nabla z_2 \\\\\n",
    "& \\\\\n",
    "& \\nabla a_1 = \\nabla z_2 \\cdot w_2^T \\\\\n",
    "& \\nabla z_1 = \\nabla a_1 \\cdot \\sigma'(z_1) \\\\\n",
    "& \\nabla b_1 = \\nabla z_1 \\\\\n",
    "& \\nabla w_1 = a_0^T \\cdot \\nabla z_1 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_z_2 = tf.multiply(diff, sigmaprime(z_2))\n",
    "d_b_2 = d_z_2\n",
    "d_w_2 = tf.matmul(tf.transpose(a_1), d_z_2)\n",
    "\n",
    "d_a_1 = tf.matmul(d_z_2, tf.transpose(w_2))\n",
    "d_z_1 = tf.multiply(d_a_1, sigmaprime(z_1))\n",
    "d_b_1 = d_z_1\n",
    "d_w_1 = tf.matmul(tf.transpose(a_0), d_z_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the network\n",
    "We take the computed $\\nabla_s$ and update the weights in one step.\n",
    "$$\n",
    "\\begin{align}\n",
    "& w_1 \\leftarrow w_1 - \\eta \\cdot \\nabla w_1 \\\\\n",
    "& b_1 \\leftarrow b_1 - \\eta \\cdot \\nabla b_1 \\\\\n",
    "& w_2 \\leftarrow w_2 - \\eta \\cdot \\nabla w_2 \\\\\n",
    "& b_2 \\leftarrow b_2 - \\eta \\cdot \\nabla b_2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "In TensorFlow, it can translate to a list of a assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta = tf.constant(0.5)\n",
    "step = [\n",
    "    tf.assign(w_1,\n",
    "            tf.subtract(w_1, tf.multiply(eta, d_w_1)))\n",
    "  , tf.assign(b_1,\n",
    "            tf.subtract(b_1, tf.multiply(eta,\n",
    "                               tf.reduce_mean(d_b_1, axis=[0]))))\n",
    "  , tf.assign(w_2,\n",
    "            tf.subtract(w_2, tf.multiply(eta, d_w_2)))\n",
    "  , tf.assign(b_2,\n",
    "            tf.subtract(b_2, tf.multiply(eta,\n",
    "                               tf.reduce_mean(d_b_2, axis=[0]))))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the training process\n",
    "The following will be able to train the network and test it in the meanwhile, using mini-batches of 10. Here, I chose to test with 1000 items from the test set, every 1000 mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurace: 4.300\n",
      "Accurace: 73.200\n",
      "Accurace: 77.600\n",
      "Accurace: 77.800\n",
      "Accurace: 86.700\n",
      "Accurace: 91.300\n",
      "Accurace: 91.100\n",
      "Accurace: 90.400\n",
      "Accurace: 92.300\n",
      "Accurace: 91.800\n"
     ]
    }
   ],
   "source": [
    "acct_mat = tf.equal(tf.argmax(a_2, 1), tf.argmax(y, 1))\n",
    "acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(10)\n",
    "    sess.run(step, feed_dict = {a_0: batch_xs,\n",
    "                                y : batch_ys})\n",
    "    if i % 1000 == 0:\n",
    "        res = sess.run(acct_res, feed_dict =\n",
    "                       {a_0: mnist.test.images[:1000],\n",
    "                        y : mnist.test.labels[:1000]})\n",
    "        print (\"Accurace: %2.3f\" % (res/10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running it shows that it manages to train the network, as we quickly get approximately 923 correct out of 1000 tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "The great part about TensorFlow is its ability to derive the step function on its own. So, instead of the rather complicated ‘Backward propagation’ and ‘Updating the network’ given above for educational purposes, we can simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.multiply(diff, diff)\n",
    "step = tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the training process (Automatic Differentiation)\n",
    "The following will be able to train the network and test it in the meanwhile, using mini-batches of 10. Here, I chose to test with 1000 items from the test set, every 1000 mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurace: 11.200\n",
      "Accurace: 75.300\n",
      "Accurace: 79.800\n",
      "Accurace: 80.700\n",
      "Accurace: 82.100\n",
      "Accurace: 80.500\n",
      "Accurace: 82.100\n",
      "Accurace: 83.000\n",
      "Accurace: 83.100\n",
      "Accurace: 83.200\n"
     ]
    }
   ],
   "source": [
    "acct_mat = tf.equal(tf.argmax(a_2, 1), tf.argmax(y, 1))\n",
    "acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(10)\n",
    "    sess.run(step, feed_dict = {a_0: batch_xs,\n",
    "                                y : batch_ys})\n",
    "    if i % 1000 == 0:\n",
    "        res = sess.run(acct_res, feed_dict =\n",
    "                       {a_0: mnist.test.images[:1000],\n",
    "                        y : mnist.test.labels[:1000]})\n",
    "        print (\"Accurace: %2.3f\" % (res/10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
